<html>
<head>
  <link rel="stylesheet" href="./plad.css">
  <script type="text/javascript" src="./plad.js"></script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML' type='text/javascript'></script>
</head>

<body>
<h1>Homework: Git Bisect</h1>

<p>
Most software projects reside in a Git repository.
One of the more useful Git commands is <tt>bisect</tt>.
The following is an example from its manual page:

<pre>
\$ git bisect start
\$ git bisect bad                 # Current version is bad
\$ git bisect good abc1234        # abc1234 is known to be good
</pre>

<p>
The current version has some bug (is <tt>bad</tt>); version <tt>abc1234</tt> does not (it is <tt>good</tt>).
The goal is to find the commit that introduced the bug.
Git will checkout some version between <tt>abc1234</tt> and the current one and then let you figure out if the bug is present.
If the bug is present, you should run the following command:

<pre>
git bisect bad
</pre>

<p>
If the bug is not present, you should run the following command:

<pre>
git bisect good
</pre>

<p>
After this, Git checks out another version, you inform Git if the version is bad or good, and this process repeats several times.
Eventually, Git reports what is the commit that introduced the bug.
Now you have a good hint about where in the code you should look.
Or, at least, you know who to blame.

<p>
Git tries to figure out where the bug was introduced by asking few questions.
After all, figuring out if the bug is present or not may be time consuming: at the very least, it involves compiling the whole project.
<a href="https://git-scm.com/docs/git-bisect-lk2009">Git-bisect uses a heuristic</a> that works on arbitrary histories, and degenerates to <a href="https://en.wikipedia.org/wiki/Binary_search_algorithm">binary search</a> when used on linear histories.
Your task is to try to do better.


<h2>A Small Example</h2>

<p>
You will write a program that communicates with a server: your program plays the role of <tt>git-bisect</tt>, the server plays the role of a human.
The communication works over <a href="https://en.wikipedia.org/wiki/WebSocket">WebSockets</a>, with messages in <a href="https://en.wikipedia.org/wiki/JSON">JSON</a> format.
We use WebSockets because, unlike sockets, they have a notion of message; that is, communication is not a stream of bytes, but a stream of messages.
We use JSON because it is a widely used standard for representing structured information in a human-readable format, which makes it easy to debug.

<p>
Let us consider an example.
Immediately after connecting, the client authenticates by sending the following message:

<pre>
  {"User": ["rg399", "TOKEN"] }
</pre>

<p>
Above, <tt>TOKEN</tt> should be replaced by the access token you received by email.

<p>
Next, the server replies with a repository description (Repo), followed by a problem instance (Instance):

<pre>
  {"Repo":{"name":"pb0","instance_count":10,"dag":[["a",[]],["b",["a"]],["c",["b"]]]}}
  {"Instance":{"good":"a","bad":"c"}}
</pre>

<p>
  The dag
  (<a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed
  acyclic graph</a>) lists the parents of each commit:
<ul>
  <li><tt>a</tt> has no parent
  <li><tt>b</tt> has parent <tt>a</tt>
  <li><tt>c</tt> has parent <tt>b</tt>
</ul>

<p>
This is a linear history, because each commit but one has exactly one parent (i.e., there is no branching).
In general, one can have <i>merge</i> commits, which have multiple parents.
Cycles, however, will never be present.

<p>
The Repo message is followed by an Instance message, which tells us that <tt>a</tt> is good and <tt>c</tt> is bad.
So, the bug could have been introduced by <tt>b</tt> or <tt>c</tt>; but, which one?

<p>
The client now asks, and the server responds:

<pre>
  {"Question":"b"}
  {"Answer":"Good"}
</pre>

<p>
And again:

<pre>
  {"Question":"c"}
  {"Answer":"Bad"}
</pre>

<p>
At this point, the client figured out that the bug was introduced in version <tt>c</tt>, and communicates its solution to the server:

<pre>
  {"Solution":"c"}
</pre>

<p>
The server now continues in one of three ways:
<ul>
<li>With a new Repo message.
<li>With a new Instance message.
  (Each Instance message refers to the last Repo message.
  The Repo message gives the dag; the Instance message gives the known good/bad commits.)
<li>With a Scores message, if the submission was successful.
</ul>

In this example, there is no further problem instance to solve, so the server replies with:

<pre>
  {"Score":{"pb0":{"Correct":2}}}
</pre>

<p>
This message says that the solution was correct, and it was found with 2 questions.
The other possible outcomes are the following:

<pre>
  {"Score":{"pb0":"Wrong"}}
  {"Score":{"pb0":"GaveUp"}}
</pre>

<p>
The last outcome is achieved by sending a <tt>"GiveUp"</tt> message instead of a question or a solution.

<p>
Observe that, from the problem instance, we already know that <tt>c</tt> is bad, so the second question asked by the client is redundant: this problem instance can be solved with just one question!

<h2>Miscellany</h2>

<p>
<b>Limits.</b>
<ul>
<li>At deadline, the server will hang up.
<li>The client may ask <i>at most 30 questions</i>; otherwise, the server hangs up and the submission does not count.
  To avoid this, the client should send a <tt>"GiveUp"</tt> message.
<li>The size of the dag (the number of strings occurring at the right of "dag") will be at most 1 million.
</ul>

<p>
<b>Deadlines, and Simple Solution.</b>
There is more information on <a href="https://moodle.kent.ac.uk/2019/course/view.php?id=5207">Moodle</a>.
In particular, you can find there what are the deadlines.
And you can download a simple Java solution and you can download a simple Java solution.
The simple solution will ask a question for each commit, but give up if the problem instance has more than 30 commits.

<p>
<b>Clarifications.</b>
For clarifications, we recommend using Piazza.
You can also just ask us.


<h2>Marking</h2>

<p>
This programming project is worth 30% of your final mark.
Because of this, the scoring rules here describe how 30 points are awarded.
(However, because of how SDS works, marks will appear there out of 100 points, then they will be weighted to give a coursework mark, they weighted again for the final mark, and after all that weighting the score gets back to something out of 30, except that small rounding errors are introduced in the process. Sigh.)

<p>
Rules:

<ul>
<li>21 points are given for correctness; 9 points are given for efficiency.
<li>
  You may assume that your submission was successful only if you receive the <b>Scores</b> message from the server.
  In particular, if you go over the question limit, or if the websocket connection times out, then the submission was <i>not</i> successful and does not count.
<li>If a solution is wrong, then not only the score for that problem is zero, but efficiency points are also zero.
  To avoid this situation, you should <tt>"GiveUp"</tt> when you cannot determine the right answer.
<li>The number of points you get for correctness will be proportional to the number of problem instances you solve correctly; for example, if there will be 1000 problem instances and you solve correctly 537 of them, then you get $537/1000 \times 21 = 11.277$ points.
<li>
  You may get efficiency points only if you do not give any wrong answer.
  Let $r_i$ be your raw score for problem instance $i$; we set $r_i=30$ if you gave up on problem instance $i$.
  The estimated raw score is $\hat r = \frac{1}{n}\sum_{i=1}^{n}r_i$; that is, the arithmetic mean of the raw scores.
  A raw score of $30$ gives you 0 efficiency points; if you have the lowest raw score of all (last) submissions you get 9 efficiency points.
  Anything in-between is linearly interpolated.
</ul>

<h2>Test Data</h2>

<p>
There are two servers: the test server and the submission server.
Only what you submit to the submission server counts towards your mark.

<p>
On each connection, you receive <i>the same repositories</i> but <i>different problem instances</i>.
Instances are generated randomly, as described below.
The test server has 100 instances, which means that scores may fluctuate.
The submission server has 10000 instances, which means that scores will be quite stable, but it also means that a real submission will take 100 times the time of a test submission.
You must remember that you need to be able to finish your real submission by the deadline.

<p>
The test server has a scoreboard visible by everyone.
The submission server does not.

<p>
The test data is generated as follows:

<ol>
  <li>For each size range out of $(0,30], (30,10^4), [10^4,10^5), [10^5,10^6)$, several git repositories are chosen.
  For the small range $(0,30]$, most repositories are synthetic;
  for the other ranges, all repositories are repositories of real software projects.
  <li> From each repository, problem instances are generated, as follows:
    <ul>
    <li>A non-root commit $b$ is chosen uniformly at random as <i>the (hidden) bug</i>.
    <li>A descendant of $b$ is chosen uniformly at random as <i>the known bad commit</i>.
    <li>A non-descendant of $b$ is chosen uniformly at random as <i>the known good commit</i>.
    </ul>
</ol>

<p>
In total, the submission server has

<ul>
<li>5000 tests with the number of commits in the range $(0,30]$,
<li>2200 tests with the number of commits in the range $(30,10^4)$,
<li>1400 tests with the number of commits in the range $[10^4,10^5)$, and
<li>1400 tests with the number of commits in the range $[10^5,10^6)$.
</ul>

<p>
The test server has 100 times fewer tests, in each size category.

<h2>References</h2>

<ol>
<li><a href="https://git-scm.com/docs/git-bisect-lk2009">Git Bisect Internals</a></li>
<li><a href="https://git-scm.com/book/en/v2/Git-Internals-Git-Objects">Git Objects</a> (commits, trees).
</ol>

<h2>Changelog</h2>

<ul>
<li><b>2020-02-13</b>
  <ul>
  <li>Drop the requirement that each server message is answered quickly.
  <li>Added the possibility to give up on a problem.
  <li>Updated score split to 21 for correctness plus 9 for efficiency.
  <li>Described how test data is generated.
  </ul>
<li><b>2020-02-14</b>: The sample solution <i>is</i> correct because it does give up.
<li><b>2020-02-26</b>: Added access tokens.
<li><b>2020-03-02</b>: Split Problem messages into Repo/Instance messages. Reduced maximum questions to 30. Updated scoring rules to depend on average number of questions. Tests are different on each connection.
</ul>

</body>
</html>
<!--
vim:spell:spelllang=en_us:wrap:linebreak:
-->
