<html>
<head>
  <link rel="stylesheet" href="./plad.css">
  <script type="text/javascript" src="./plad.js"></script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML' type='text/javascript'></script>
</head>

<body>
<h1>Homework: Git Bisect</h1>

<p>
Most software projects reside in a Git repository.
One of the more useful Git commands is <tt>bisect</tt>.
The following is an example from its manual page:

<pre>
\$ git bisect start
\$ git bisect bad                 # Current version is bad
\$ git bisect good abc1234        # abc1234 is known to be good
</pre>

<p>
The current version has some bug (is <tt>bad</tt>); version <tt>abc1234</tt> does not (it is <tt>good</tt>).
The goal is to find the commit that introduced the bug.
Git will checkout some version between <tt>abc1234</tt> and the current one and then let you figure out if the bug is present.
If the bug is present, you should run the following command:

<pre>
git bisect bad
</pre>

<p>
If the bug is not present, you should run the following command:

<pre>
git bisect good
</pre>

<p>
After this, Git checks out another version, you inform Git if the version is bad or good, and this process repeats several times.
Eventually, Git reports what is the commit that introduced the bug.
Now you have a good hint about where in the code you should look.
Or, at least, you know who to blame.

<p>
Git tries to figure out where the bug was introduced by asking few questions.
After all, figuring out if the bug is present or not may be time consuming: at the very least, it involves compiling the whole project.
<a href="https://git-scm.com/docs/git-bisect-lk2009">Git-bisect uses a heuristic</a> that works on arbitrary histories, and degenerates to <a href="https://en.wikipedia.org/wiki/Binary_search_algorithm">binary search</a> when used on linear histories.
Your task is to try to do better.


<h2>A Small Example</h2>

<p>
You will write a program that communicates with a server: your program plays the role of <tt>git-bisect</tt>, the server plays the role of a human.
The communication works over <a href="https://en.wikipedia.org/wiki/WebSocket">WebSockets</a>, with messages in <a href="https://en.wikipedia.org/wiki/JSON">JSON</a> format.
We use WebSockets because, unlike sockets, they have a notion of message; that is, communication is not a stream of bytes, but a stream of messages.
We use JSON because it is a widely used standard for representing structured information in a human-readable format, which makes it easy to debug.

<p>
Let us consider an example.
Immediately after connecting, the client authenticates by sending the following message:

<pre>
  {"User": "rg399" }
</pre>

<p>
(There is no password, but submitting solutions on behalf of somebody else will be considered a breach of academic integrity.)

<p>
Next, the server replies with a problem instance:

<pre>
  {"Problem":{"name":"pb0","good":"a","bad":"c","dag":[["a",[]],["b",["a"]],["c",["b"]]]}}
</pre>

<p>
  The dag
  (<a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed
  acyclic graph</a>) lists the parents of each commit:
<ul>
  <li><tt>a</tt> has no parent
  <li><tt>b</tt> has parent <tt>a</tt>
  <li><tt>c</tt> has parent <tt>b</tt>
</ul>

<p>
This is a linear history, because each commit but one has exactly one parent (i.e., there is no branching).
In general, one can have <i>merge</i> commits, which have multiple parents.
Cycles, however, will never be present.
The message also tells us that <tt>a</tt> is good and <tt>c</tt> is bad.
So, the bug could have been introduced by <tt>b</tt> or <tt>c</tt>; but, which one?

<p>
The client now asks, and the server responds:

<pre>
  {"Question":"b"}
  {"Answer":"Good"}
</pre>

<p>
And again:

<pre>
  {"Question":"c"}
  {"Answer":"Bad"}
</pre>

<p>
At this point, the client figured out that the bug was introduced in version <tt>c</tt>, and communicates its solution to the server:

<pre>
  {"Solution":"c"}
</pre>

<p>
The server now continues by either presenting a new problem instance, or by providing raw scores for all the problem instances seen so far.
In this example, there is no further problem instance to solve, so the server replies with:

<pre>
  {"Score":{"pb0":2}}
</pre>

<p>
If the solution was correct, the raw score is simply the number of questions asked;
if the solution was incorrect, the raw score will be the special value <tt>null</tt>.
The <tt>null</tt> score is also given when the client gives up, which can be done by sending the message <tt>"GiveUp"</tt> instead of a question or a solution.

<p>
Observe that, from the problem instance, we already know that <tt>c</tt> is bad, so the second question asked by the client is redundant: this problem instance can be solved with just one question!

<h2>Miscellany</h2>

<p>
<b>Limits.</b>
<ul>
<li>At deadline, the server will hang up.
<li>The client may ask <i>at most 1000 questions</i>; otherwise, the server hangs up and the submission does not count.
  To avoid this, the client should send a <tt>"GiveUp"</tt> message.
<li>The size of the dag (the number of strings occurring at the right of "dag") will be at most 10 million.
</ul>

<p>
<b>Deadlines, and Simple Solution.</b>
There is more information on <a href="https://moodle.kent.ac.uk/2019/course/view.php?id=5207">Moodle</a>.
In particular, you can find there what are the deadlines.
And you can download a simple Java solution and you can download a simple Java solution.
The simple solution will ask a question for each commit, and is therefore incorrect for problem instances that have more than 1000 commits.

<p>
<b>Clarifications.</b>
For clarifications, we recommend using Piazza.
You can also just ask us.


<h2>Marking</h2>

<p>
This programming project is worth 30% of your final mark.
Because of this, the scoring rules here describe how 30 points are awarded.
(However, because of how SDS works, marks will appear there out of 100 points, then they will be weighted to give a coursework mark, they weighted again for the final mark, and after all that weighting the score gets back to something out of 30, except that small rounding errors are introduced in the process. Sigh.)

<p>
Rules:

<ul>
<li>21 points are given for correctness; 9 points are given for efficiency.
<li>
  You may assume that your submission was successful only if you receive the <b>Scores</b> message from the server.
  In particular, if you go over the question limit, or if the websocket connection times out, then the submission was <i>not</i> successful and does not count.
<li>If a solution is wrong, then not only the score for that problem is zero, but efficiency points are also zero.
  To avoid this situation, you should <tt>"GiveUp"</tt> when you cannot determine the right answer.
<li>The number of points you get for correctness will be proportional to the number of problem instances you solve correctly; for example, if there will be 1000 problem instances and you solve correctly 537 of them, then you get $537/1000 \times 21 = 11.277$ points.
<li>
  You may get efficiency points only if you do not give any wrong answer.
  Let $r_i$ be your raw score for problem instance $i$; we set $r_i=1000$ if you gave up on problem instance $i$.
  Let $b_i$ be the best known raw score for problem instance $i$.
  If there are $n$ problem instances, then your score for efficiency is $9 \sqrt[n]{\prod_{i=1}^n \frac{b_i}{r_i}}$; that is, it is the geometric mean of the relative scores $b_i/r_i$, scaled to 9.
</ul>

<h2>Test Data</h2>

<p>
There are two servers: the test server and the submission server.
Only what you submit to the submission server counts towards your mark.
However, using the submission server is fraught with dangers:

<ul>
<li><b>Only your first submission counts.</b>
  Otherwise, one could (a)&nbsp;identify the bug in one submission and (b)&nbsp;submit again and only ask the minimum number of questions <i>given that you already know where the bug is</i>.
  The server will, however, accept multiple submissions.
  The latter ones will be taken into account only if there are special circumstances that are explained in an email.
<li><b>The first time you connect to the server, you will most likely discover that your code has some bug and the submission will fail.</b>
<li><b>The submission server is available for 48 hours only.</b>
</ul>

<p>
To alleviate these problems, a test server is provided.
For the test server, only the <i>last</i> submission counts.
A scoreboard is provided, which updates roughly every minute.
The test server has 100 tests while the submission server has 1000 tests:

<ul>
<li>
  Making a real submission will be 10 times slower than making a test submission.
  You must remember that you need to be able to finish your real submission by the deadline.
<li>
  Your mark will be more precise than what you see on the test server scoreboard.
<li>
  You should pay close attention to <i>not</i> overfit your solution to the test server.
</ul>

<p>
The test data is generated as follows:

<ol>
  <li>For each size range out of $(0,10^3), [10^3,10^4), [10^4,10^5), [10^5,10^6)$, several real git repositories are chosen.
  (Sizes here are number of commits.)
  Examples:
    <ul>
    <li>Overleaf for $(0,10^3)$
    <li>React for $[10^3,10^4)$
    <li>Bootstrap for $[10^4,10^5)$
    <li>Linux for $[10^5,10^6)$
    </ul>
  <li> From each repository, a number of problem instances are generated, as follows:
    <ul>
    <li>A non-root commit $b$ is chosen uniformly at random as <i>the (hidden) bug</i>.
    <li>A descendant of $b$ is chosen uniformly at random as <i>the known bad commit</i>.
    <li>A non-descendant of $b$ is chosen uniformly at random as <i>the known good commit</i>.
    </ul>
</ol>

<p>
In total, the submission server has

<ul>
<li>500 tests with the number of commits in the range $(0,10^3)$,
<li>220 tests with the number of commits in the range $[10^3,10^4)$,
<li>140 tests with the number of commits in the range $[10^4,10^5)$, and
<li>140 tests with the number of commits in the range $[10^5,10^6)$.
</ul>

<p>
The test server has 10 times fewer tests, in each size category.

<h2>References</h2>

<ol>
<li><a href="https://git-scm.com/docs/git-bisect-lk2009">Git Bisect Internals</a></li>
<li><a href="https://git-scm.com/book/en/v2/Git-Internals-Git-Objects">Git Objects</a> (commits, trees).
</ol>

<h2>Changelog</h2>

<ul>
<li><b>2020-02-13</b>
  <ul>
  <li>Drop the requirement that each server message is answered quickly.
  <li>Added the possibility to give up on a problem.
  <li>Updated score split to 21 for correctness plus 9 for efficiency.
  <li>Described how test data is generated.
  </ul
</ul>

</body>
</html>
<!--
vim:spell:spelllang=en_us:wrap:linebreak:
-->
